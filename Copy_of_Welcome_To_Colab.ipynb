{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def mytokenizer(corpus):\n",
        "    newcorpus = corpus + \"$\"\n",
        "    words = []\n",
        "    tmp = \"\"\n",
        "    for i in range(len(newcorpus)):\n",
        "        if newcorpus[i] in [\" \", \"$\", \".\", \"\\n\"]:\n",
        "            if len(tmp) > 0:\n",
        "                words.append(tmp)\n",
        "                tmp = \"\"\n",
        "        else:\n",
        "            tmp = tmp + newcorpus[i]\n",
        "    return words\n",
        "\n",
        "def text_filter(tokens):\n",
        "    punc = \"!()-[]{};:'\\\"\\\\,<>./?@#$%^&*_~।\"\n",
        "    clean_tokens = [w.strip(punc) for w in tokens]\n",
        "    clean_tokens = [tx for tx in clean_tokens if not tx.isdigit()]\n",
        "    digits = \"0123456789\"\n",
        "    clean_tokens = [w.strip(digits) for w in clean_tokens]\n",
        "    return clean_tokens\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    stop_words = [\"i\", \"are\", \"we\", \"is\", \"to\", \"and\", \"will\", \"shall\", \"should\", \"they\", \"him\", \"he\", \"her\"]\n",
        "    return [w for w in tokens if w not in stop_words]\n",
        "\n",
        "def my_stemmer(tokens):\n",
        "    return [re.sub(r'less|ship|ing|les|ly|es|s|ity|ness|ed|ies', '', word) for word in tokens]\n",
        "\n",
        "def validate_script(text):\n",
        "    for char in text:\n",
        "        script = unicodedata.name(char)\n",
        "        if script != 'Common':\n",
        "            return script\n",
        "    return None\n",
        "\n",
        "sentence = \"Pointing to the close123 123 #@relations developed with India by the Awami League government, Bangladesh Information Minister Hasan Mahmud pointed out that the country's ties with its larger neighbour have always been affected and minorities have faced atrocities whenever the opposition BNP comes to power.\"\n",
        "\n",
        "tokens = mytokenizer(sentence)\n",
        "filtered_tokens = text_filter(tokens)\n",
        "new_filtered_tokens = remove_stopwords(filtered_tokens)\n",
        "stem_words = my_stemmer(new_filtered_tokens)\n",
        "\n",
        "print(\"The tokens are\")\n",
        "print(tokens)\n",
        "print(\"Filtered tokens:\")\n",
        "print(filtered_tokens)\n",
        "print(\"The tokens without stop words:\")\n",
        "print(new_filtered_tokens)\n",
        "print(\"Stemmed Words\")\n",
        "print(stem_words)\n",
        "\n",
        "text = \"नमस्ते\"\n",
        "script = validate_script(text)\n",
        "if script:\n",
        "    print(f\"The text is in {script} script.\")\n",
        "else:\n",
        "    print(\"Unable to determine the script.\")\n"
      ],
      "metadata": {
        "id": "FiAo7UfJmJk6",
        "outputId": "9ce0d254-9a78-4483-c78b-6c28f46d9c89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tokens are\n",
            "['Pointing', 'to', 'the', 'close123', '123', '#@relations', 'developed', 'with', 'India', 'by', 'the', 'Awami', 'League', 'government,', 'Bangladesh', 'Information', 'Minister', 'Hasan', 'Mahmud', 'pointed', 'out', 'that', 'the', \"country's\", 'ties', 'with', 'its', 'larger', 'neighbour', 'have', 'always', 'been', 'affected', 'and', 'minorities', 'have', 'faced', 'atrocities', 'whenever', 'the', 'opposition', 'BNP', 'comes', 'to', 'power']\n",
            "Filtered tokens:\n",
            "['Pointing', 'to', 'the', 'close', 'relations', 'developed', 'with', 'India', 'by', 'the', 'Awami', 'League', 'government', 'Bangladesh', 'Information', 'Minister', 'Hasan', 'Mahmud', 'pointed', 'out', 'that', 'the', \"country's\", 'ties', 'with', 'its', 'larger', 'neighbour', 'have', 'always', 'been', 'affected', 'and', 'minorities', 'have', 'faced', 'atrocities', 'whenever', 'the', 'opposition', 'BNP', 'comes', 'to', 'power']\n",
            "The tokens without stop words:\n",
            "['Pointing', 'the', 'close', 'relations', 'developed', 'with', 'India', 'by', 'the', 'Awami', 'League', 'government', 'Bangladesh', 'Information', 'Minister', 'Hasan', 'Mahmud', 'pointed', 'out', 'that', 'the', \"country's\", 'ties', 'with', 'its', 'larger', 'neighbour', 'have', 'always', 'been', 'affected', 'minorities', 'have', 'faced', 'atrocities', 'whenever', 'the', 'opposition', 'BNP', 'comes', 'power']\n",
            "Stemmed Words\n",
            "['Point', 'the', 'cloe', 'relation', 'develop', 'with', 'India', 'by', 'the', 'Awami', 'League', 'government', 'Bangladh', 'Information', 'Miniter', 'Haan', 'Mahmud', 'point', 'out', 'that', 'the', \"country'\", 't', 'with', 'it', 'larger', 'neighbour', 'have', 'alway', 'been', 'affect', 'minorit', 'have', 'fac', 'atrocit', 'whenever', 'the', 'oppoition', 'BNP', 'com', 'power']\n",
            "The text is in DEVANAGARI LETTER NA script.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "def mytokenizer(corpus):\n",
        "    newcorpus = corpus + \"$\"\n",
        "    words = []\n",
        "    tmp = \"\"\n",
        "    for i in range(len(newcorpus)):\n",
        "        if newcorpus[i] in [\" \", \"$\", \".\", \"\\n\"]:\n",
        "            if len(tmp) > 0:\n",
        "                words.append(tmp)\n",
        "                tmp = \"\"\n",
        "        else:\n",
        "            tmp = tmp + newcorpus[i]\n",
        "    return words\n",
        "def text_filter(tokens):\n",
        "    punc = \"!()-[]{};:'\\\"\\\\,<>./?@#$%^&*_~।\"\n",
        "    clean_tokens = [w.strip(punc) for w in tokens]\n",
        "    clean_tokens = [tx for tx in clean_tokens if not tx.isdigit()]\n",
        "    digits = \"0123456789\"\n",
        "    clean_tokens = [w.strip(digits) for w in clean_tokens]\n",
        "    return clean_tokens\n",
        "def remove_stopwords(tokens):\n",
        "    stop_words = [\"i\", \"are\", \"we\", \"is\", \"to\", \"and\", \"will\", \"shall\", \"should\", \"they\", \"him\", \"he\", \"her\"]\n",
        "    return [w for w in tokens if w not in stop_words]\n",
        "def my_stemmer(tokens):\n",
        "    return [re.sub(r'less|ship|ing|les|ly|es|s|ity|ness|ed|ies', '', word) for word in tokens]\n",
        "def validate_script(text):\n",
        "    for char in text:\n",
        "        script = unicodedata.name(char)\n",
        "        if script != 'Common':\n",
        "            return script\n",
        "    return None\n",
        "\n",
        "sentence = \"Pointing to the close123 123 #@relations developed with India by the Awami League government, Bangladesh Information Minister Hasan Mahmud pointed out that the country's ties with its larger neighbour have always been affected and minorities have faced atrocities whenever the opposition BNP comes to power.\"\n",
        "\n",
        "tokens = mytokenizer(sentence)\n",
        "filtered_tokens = text_filter(tokens)\n",
        "new_filtered_tokens = remove_stopwords(filtered_tokens)\n",
        "stem_words = my_stemmer(new_filtered_tokens)\n",
        "\n",
        "print(\"The tokens are\")\n",
        "print(tokens)\n",
        "print(\"Filtered tokens:\")\n",
        "print(filtered_tokens)\n",
        "print(\"The tokens without stop words:\")\n",
        "print(new_filtered_tokens)\n",
        "print(\"Stemmed Words\")\n",
        "print(stem_words)\n",
        "\n",
        "text = \"नमस्ते\"\n",
        "script = validate_script(text)\n",
        "if script:\n",
        "    print(f\"The text is in {script} script.\")\n",
        "else:\n",
        "    print(\"Unable to determine the script.\")\n"
      ],
      "metadata": {
        "id": "5Lm56xAzmUn2",
        "outputId": "d690f2a8-00d3-4b64-e033-181aafe99705",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tokens are\n",
            "['Pointing', 'to', 'the', 'close123', '123', '#@relations', 'developed', 'with', 'India', 'by', 'the', 'Awami', 'League', 'government,', 'Bangladesh', 'Information', 'Minister', 'Hasan', 'Mahmud', 'pointed', 'out', 'that', 'the', \"country's\", 'ties', 'with', 'its', 'larger', 'neighbour', 'have', 'always', 'been', 'affected', 'and', 'minorities', 'have', 'faced', 'atrocities', 'whenever', 'the', 'opposition', 'BNP', 'comes', 'to', 'power']\n",
            "Filtered tokens:\n",
            "['Pointing', 'to', 'the', 'close', 'relations', 'developed', 'with', 'India', 'by', 'the', 'Awami', 'League', 'government', 'Bangladesh', 'Information', 'Minister', 'Hasan', 'Mahmud', 'pointed', 'out', 'that', 'the', \"country's\", 'ties', 'with', 'its', 'larger', 'neighbour', 'have', 'always', 'been', 'affected', 'and', 'minorities', 'have', 'faced', 'atrocities', 'whenever', 'the', 'opposition', 'BNP', 'comes', 'to', 'power']\n",
            "The tokens without stop words:\n",
            "['Pointing', 'the', 'close', 'relations', 'developed', 'with', 'India', 'by', 'the', 'Awami', 'League', 'government', 'Bangladesh', 'Information', 'Minister', 'Hasan', 'Mahmud', 'pointed', 'out', 'that', 'the', \"country's\", 'ties', 'with', 'its', 'larger', 'neighbour', 'have', 'always', 'been', 'affected', 'minorities', 'have', 'faced', 'atrocities', 'whenever', 'the', 'opposition', 'BNP', 'comes', 'power']\n",
            "Stemmed Words\n",
            "['Point', 'the', 'cloe', 'relation', 'develop', 'with', 'India', 'by', 'the', 'Awami', 'League', 'government', 'Bangladh', 'Information', 'Miniter', 'Haan', 'Mahmud', 'point', 'out', 'that', 'the', \"country'\", 't', 'with', 'it', 'larger', 'neighbour', 'have', 'alway', 'been', 'affect', 'minorit', 'have', 'fac', 'atrocit', 'whenever', 'the', 'oppoition', 'BNP', 'com', 'power']\n",
            "The text is in DEVANAGARI LETTER NA script.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_max_length_token(corpus):\n",
        "    tokens = corpus.split()\n",
        "    max_token = max(tokens, key=len) if tokens else \"\"\n",
        "    return max_token\n",
        "\n",
        "corpus = \"Pointing to the close123 123 #@relations developed with India by the Awami League government\"\n",
        "max_length_token = find_max_length_token(corpus)\n",
        "print(\"Maximum length token:\", max_length_token)\n"
      ],
      "metadata": {
        "id": "g4Nn07Gkm5Ao",
        "outputId": "fa8de14e-c218-4622-ae23-7ca5c14739a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum length token: #@relations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_custom_stopwords(corpus, stop_words):\n",
        "    tokens = corpus.split()\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    return \" \".join(filtered_tokens)\n",
        "\n",
        "custom_stopwords = {\n",
        "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\",\n",
        "    \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\",\n",
        "    \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\",\n",
        "    \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\",\n",
        "    \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n",
        "    \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\"\n",
        "}\n",
        "\n",
        "corpus = \"I am writing this code to demonstrate how to remove custom stopwords from a given text.\"\n",
        "filtered_corpus = remove_custom_stopwords(corpus, custom_stopwords)\n",
        "\n",
        "print(\"Original Corpus:\")\n",
        "print(corpus)\n",
        "print(\"\\nFiltered Corpus:\")\n",
        "print(filtered_corpus)\n"
      ],
      "metadata": {
        "id": "Fw77yhBNn9VW",
        "outputId": "8a1780a4-804a-47ba-aa49-832df3f98fd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Corpus:\n",
            "I am writing this code to demonstrate how to remove custom stopwords from a given text.\n",
            "\n",
            "Filtered Corpus:\n",
            "writing code to demonstrate how to remove custom stopwords from a given text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "\n",
        "words = ['cared', 'university', 'fairly', 'easily', 'singing',\n",
        "         'sings', 'sung', 'singer', 'sportingly', 'magnified']\n",
        "\n",
        "st = LancasterStemmer()\n",
        "stem_words = [st.stem(w) for w in words]\n",
        "\n",
        "print(\"Lancaster Stemmer\")\n",
        "for original, stemmed in zip(words, stem_words):\n",
        "    print(original + ' ----> ' + stemmed)\n"
      ],
      "metadata": {
        "id": "KblNLKZloeWF",
        "outputId": "426285a8-ce23-42c5-fdee-91181c4a75cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lancaster Stemmer\n",
            "cared ----> car\n",
            "university ----> univers\n",
            "fairly ----> fair\n",
            "easily ----> easy\n",
            "singing ----> sing\n",
            "sings ----> sing\n",
            "sung ----> sung\n",
            "singer ----> sing\n",
            "sportingly ----> sport\n",
            "magnified ----> magn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = ['cared', 'university', 'fairly', 'easily', 'singing',\n",
        "         'sings', 'sung', 'singer', 'sportingly', 'magnified']\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(w, pos=\"v\") for w in words]\n",
        "\n",
        "for original, lemmatized in zip(words, lemmatized_words):\n",
        "    print(original + ' ----> ' + lemmatized)\n"
      ],
      "metadata": {
        "id": "aIxiMIZTo6Zz",
        "outputId": "ebb69574-ad4f-415d-efea-40deffafbfe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cared ----> care\n",
            "university ----> university\n",
            "fairly ----> fairly\n",
            "easily ----> easily\n",
            "singing ----> sing\n",
            "sings ----> sing\n",
            "sung ----> sing\n",
            "singer ----> singer\n",
            "sportingly ----> sportingly\n",
            "magnified ----> magnify\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def find_suffixes(word, suffix_list):\n",
        "    matched_suffixes = [suffix for suffix in suffix_list if re.search(f\"{suffix}$\", word)]\n",
        "    return matched_suffixes\n",
        "\n",
        "suffixes = [\"ing\", \"ed\", \"ly\", \"es\", \"s\", \"ness\", \"ment\", \"able\", \"ion\", \"ity\"]\n",
        "\n",
        "word = \"magnified\"\n",
        "matched_suffixes = find_suffixes(word, suffixes)\n",
        "\n",
        "print(\"Word:\", word)\n",
        "print(\"Suffixes found:\", matched_suffixes if matched_suffixes else \"No suffixes found\")\n"
      ],
      "metadata": {
        "id": "LckxEEGEpc6S",
        "outputId": "4ea09a71-b4ba-485c-a379-843564eda951",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: magnified\n",
            "Suffixes found: ['ed']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "class NgramModel:\n",
        "    def __init__(self, n, k=1):\n",
        "        self.n = n\n",
        "        self.k = k\n",
        "        self.ngram_counts = defaultdict(int)\n",
        "        self.context_counts = defaultdict(int)\n",
        "\n",
        "    def train(self, corpus):\n",
        "        for sentence in corpus:\n",
        "            words = sentence.split()\n",
        "            for i in range(len(words) - self.n + 1):\n",
        "                ngram = tuple(words[i:i + self.n - 1])\n",
        "                next_word = words[i + self.n - 1]\n",
        "                self.ngram_counts[(ngram, next_word)] += 1\n",
        "                self.context_counts[ngram] += 1\n",
        "\n",
        "    def probability(self, ngram, next_word):\n",
        "        context_count = self.context_counts[ngram]\n",
        "        ngram_count = self.ngram_counts[(ngram, next_word)]\n",
        "        unique_words = len(set(word for _, word in self.ngram_counts.keys()))\n",
        "        return (ngram_count + self.k) / (context_count + self.k * unique_words) if context_count > 0 else 0\n",
        "\n",
        "    def generate_next_word(self, ngram):\n",
        "        possible_next_words = [word for (ngram_, word) in self.ngram_counts.keys() if ngram_ == ngram]\n",
        "        probabilities = [self.probability(ngram, word) for word in possible_next_words]\n",
        "\n",
        "        if not possible_next_words or sum(probabilities) == 0:\n",
        "            return None\n",
        "\n",
        "        probabilities = np.array(probabilities) / sum(probabilities)\n",
        "        return np.random.choice(possible_next_words, p=probabilities)\n",
        "\n",
        "corpus = [\"this is a cat\", \"this is a dog\", \"this is a cat\"]\n",
        "ngram_model = NgramModel(n=2, k=1)\n",
        "ngram_model.train(corpus)\n",
        "\n",
        "ngram = (\"is\",)\n",
        "next_word = ngram_model.generate_next_word(ngram)\n",
        "\n",
        "print(next_word)\n"
      ],
      "metadata": {
        "id": "RY5L_mTwqg1P",
        "outputId": "b9ae6473-70e3-4dc2-c0ad-a6c674cb9b43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "class NgramModel:\n",
        "    def __init__(self, n):\n",
        "        self.n = n\n",
        "        self.ngram_counts = defaultdict(int)\n",
        "        self.context_counts = defaultdict(int)\n",
        "\n",
        "    def train(self, corpus):\n",
        "        for sentence in corpus:\n",
        "            words = sentence.split()\n",
        "            for i in range(len(words) - self.n + 1):\n",
        "                ngram = tuple(words[i:i + self.n])\n",
        "                context = tuple(words[i:i + self.n - 1])\n",
        "                self.ngram_counts[ngram] += 1\n",
        "                self.context_counts[context] += 1\n",
        "\n",
        "    def get_ngram_frequencies(self):\n",
        "        return dict(self.ngram_counts)\n",
        "\n",
        "    def get_ngram_probabilities(self):\n",
        "        probabilities = {}\n",
        "        for ngram, count in self.ngram_counts.items():\n",
        "            context = ngram[:-1]\n",
        "            context_count = self.context_counts[context]\n",
        "            probabilities[ngram] = count / context_count if context_count > 0 else 0\n",
        "        return probabilities\n",
        "\n",
        "corpus = [\"this is a cat\", \"this is a dog\", \"this is a cat\"]\n",
        "n = 2\n",
        "ngram_model = NgramModel(n)\n",
        "ngram_model.train(corpus)\n",
        "ngram_frequencies = ngram_model.get_ngram_frequencies()\n",
        "ngram_probabilities = ngram_model.get_ngram_probabilities()\n",
        "print(\"N-gram Frequencies:\")\n",
        "for ngram, freq in ngram_frequencies.items():\n",
        "    print(f\"{ngram}: {freq}\")\n",
        "print(\"\\nN-gram Probabilities:\")\n",
        "for ngram, prob in ngram_probabilities.items():\n",
        "    print(f\"{ngram}: {prob:.4f}\")\n"
      ],
      "metadata": {
        "id": "M4qFMSz6qow8",
        "outputId": "466b2b72-46b3-4a70-85b6-6b2531d2be4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N-gram Frequencies:\n",
            "('this', 'is'): 3\n",
            "('is', 'a'): 3\n",
            "('a', 'cat'): 2\n",
            "('a', 'dog'): 1\n",
            "\n",
            "N-gram Probabilities:\n",
            "('this', 'is'): 1.0000\n",
            "('is', 'a'): 1.0000\n",
            "('a', 'cat'): 0.6667\n",
            "('a', 'dog'): 0.3333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "for word, tag in pos_tags:\n",
        "    print(f\"Word: {word}, POS Tag: {tag}\")\n"
      ],
      "metadata": {
        "id": "FYKjzf6IsxXV",
        "outputId": "e356f18b-e8f5-4c72-f17f-3a236e042fe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-1247415dfad4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The quick brown fox jumps over the lazy dog.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mpos_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_tags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \"\"\"\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load, lang)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_conf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_tagdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mload_from_json\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# Automatically find path to the tagger if location is not specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"taggers/averaged_perceptron_tagger_{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mTAGGER_JSONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog and runs into the forest.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "nouns = [word for word, tag in pos_tags if tag.startswith('NN')]\n",
        "verbs = [word for word, tag in pos_tags if tag.startswith('VB')]\n",
        "\n",
        "print(\"Nouns:\", nouns)\n",
        "print(\"Verbs:\", verbs)\n"
      ],
      "metadata": {
        "id": "cwH4WjFEtR54",
        "outputId": "60ecca89-6a5a-454e-92d3-09a51c041113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 863
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-e8654a4aca2b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The quick brown fox jumps over the lazy dog and runs into the forest.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mpos_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mnouns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_tags\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \"\"\"\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load, lang)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_conf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_tagdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mload_from_json\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# Automatically find path to the tagger if location is not specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"taggers/averaged_perceptron_tagger_{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mTAGGER_JSONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "# Download the necessary NLTK data package\n",
        "nltk.download('averaged_perceptron_tagger_eng')  # Download the tagger resource\n",
        "nltk.download('punkt')  # Download the tokenizer resource\n",
        "\n",
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "chunking_grammar = r\"\"\"\n",
        "    NP: {<DT|JJ|NN.*>+}\n",
        "    VP: {<VB.*><NP|PP>*}\n",
        "\"\"\"\n",
        "\n",
        "chunk_parser = RegexpParser(chunking_grammar)\n",
        "chunks = chunk_parser.parse(pos_tags)\n",
        "chunks.pretty_print()"
      ],
      "metadata": {
        "id": "YoxClZ4TtqWS",
        "outputId": "163b8eaf-cd51-40ed-fdbd-ad5cfb4af08f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                               S                                           \n",
            "    ___________________________|________________________________            \n",
            "   |     |            NP                        VP              NP         \n",
            "   |     |     _______|________________         |        _______|______     \n",
            "over/IN ./. The/DT quick/JJ brown/NN fox/NN jumps/VBZ the/DT lazy/JJ dog/NN\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "corpus = \"\"\"The quick brown fox jumps over the lazy dog.\n",
        "            A smart cat watches carefully.\n",
        "            The little boy runs quickly to school.\"\"\"\n",
        "\n",
        "tokens = word_tokenize(corpus)\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "chunking_grammar = r\"\"\"\n",
        "    NP: {<DT|JJ|NN.*>+}\n",
        "    VP: {<VB.*><NP|PP>*}\n",
        "\"\"\"\n",
        "\n",
        "chunk_parser = RegexpParser(chunking_grammar)\n",
        "chunks = chunk_parser.parse(pos_tags)\n",
        "chunks.pretty_print()\n"
      ],
      "metadata": {
        "id": "bdTTfTCeuP9d",
        "outputId": "df36e849-5155-4fd2-cc64-d1027e804655",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                      S                                                                                                      \n",
            "    __________________________________________________________________________________|_________________________________________________________________________________________________      \n",
            "   |     |       |        |      |        |    |            NP                        VP              NP                  NP                                  NP              VP        NP   \n",
            "   |     |       |        |      |        |    |     _______|________________         |        _______|______       ______|_________________          ________|_______        |         |     \n",
            "over/IN ./. carefully/RB ./. quickly/RB to/TO ./. The/DT quick/JJ brown/NN fox/NN jumps/VBZ the/DT lazy/JJ dog/NN A/DT smart/JJ cat/NN watches/NNS The/DT little/JJ boy/NN runs/VBZ school/NN\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}